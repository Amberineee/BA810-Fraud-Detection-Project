---
title: 'Team 5: Project Deliverable'
author: "Mohsen Jafari, Jeffrey Leung, Yi-shuan Wang, Ying Wu, Yuzhe Zheng"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup
```{r}
library(data.table)
library(caTools)
library(xgboost)
library(caret)
library(ROCR)
library("pROC")

credit_card_raw = fread("~/Desktop/810supervised/team project/creditcard.csv")
```

## Splitting Data
```{r}
# Create train and test dataset
credit_card_raw[, test:=0]
credit_card_raw[, "Time":= NULL]
credit_card_raw[sample(nrow(credit_card_raw), 284807*0.2), test:=1]
test <- credit_card_raw[test==1]
train <- credit_card_raw[test==0]
train[, "test" := NULL]
test[, "test" := NULL]
credit_card_raw[, "test" := NULL]

# Convert datatables to dataframes for downsampling
setDF(train)
setDF(test)

# Downsample
set.seed(1)
train$Class <- factor(train$Class)
downsample.train <- downSample(train[, -ncol(train)], train$Class)

test$Class <- factor(test$Class)
downsample.test <- downSample(test[, -ncol(test)], test$Class)
```

## Logistic Regression
```{r}
# Fit logistic regression model
set.seed(1)

down_fit <- glm(Class ~ .,family = "binomial" ,data = downsample.train)
summary(down_fit,)

pred_down <- predict(down_fit, newdata = test)
print('Fitting downsampled model to test data')

#Evaluate model performance on test set
roc.curve(test$Class, pred_down, plotit = TRUE)

```

## Decision Tree
```{r}
# Fit decision tree model
#apply 5-folds cross validation to find the best parameter cp for decision tree
ctrl <- trainControl(method = "cv", number = 5)

dt <- train(Class ~ ., data = downsample.train,
               method = 'rpart',
               trControl = ctrl)

#find best cp for decision model
#the best model is about cp = 0.015
#evaluate the best model using test data
pred <- predict(dt, downsample.test)

#performance
confusionMatrix(pred, downsample.test$Class, positive = '1')

#ROC curve
prediction_for_roc_curve <- predict(dt,downsample.test[,-ncol(downsample.test)],type="prob")
pretty_colours <- c("#F8766D","#00BA38")
classes <- levels(test$Class)

for (i in 1:2)
{
  # Define which observations belong to class[i]
  true_values <- ifelse(downsample.test[,ncol(downsample.test)]==classes[i],1,0)
  # Assess the performance of classifier for class[i]
  pred <- prediction(prediction_for_roc_curve[,i],true_values)
  perf <- performance(pred, "tpr", "fpr")
  if (i==1)
  {
    plot(perf,main="ROC Curve",col=pretty_colours[i]) 
  }
  else
  {
    plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE) 
  }
   # Calculate the AUC and print it to screen
  auc.perf <- performance(pred, measure = "auc")
  print(auc.perf@y.values)
}

```

## Random Forest
```{r}
# Fit random forest model
fit_rndfor <- randomForest(downsample.train$Class~., data=downsample.train, ntree = 500, importance = TRUE)

varImpPlot(fit_rndfor)

#make predictions 
pd <- predict(fit_rndfor, downsample.train[,-ncol(downsample.train)])
table(observed = downsample.train[,ncol(downsample.train)], predicted = pd)

pd.test <- predict(fit_rndfor, downsample.test[,-ncol(downsample.test)])
table(observed = downsample.test[,ncol(downsample.test)], predicted = pd.test)

#ROC curve
prediction_for_roc_curve <- predict(fit_rndfor,downsample.test[,-ncol(downsample.test)],type="prob")
pretty_colours <- c("#F8766D","#00BA38")
classes <- levels(test$Class)

for (i in 1:2)
{
  # Define which observations belong to class[i]
  true_values <- ifelse(downsample.test[,ncol(downsample.test)]==classes[i],1,0)
  # Assess the performance of classifier for class[i]
  pred <- prediction(prediction_for_roc_curve[,i],true_values)
  perf <- performance(pred, "tpr", "fpr")
  if (i==1)
  {
    plot(perf,main="ROC Curve",col=pretty_colours[i]) 
  }
  else
  {
    plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE) 
  }
  # Calculate the AUC and print it to screen
  auc.perf <- performance(pred, measure = "auc")
  print(auc.perf@y.values)
}

#cross validation 
rf.cv <- rfcv(downsample.train[,-ncol(downsample.train)], downsample.train[,ncol(downsample.train)], cv.fold = 10) 
with(rf.cv, plot(n.var,error.cv))

#tuning parameter
bestmtry <- tuneRF(downsample.train[,-ncol(downsample.train)], downsample.train[,ncol(downsample.train)], stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry) #mtry = 4 gives an accuracy of 94%; mtry: Number of variables randomly sampled as candidates at each split.
```

## XGBoost
```{r}
# Fit XGBoost model
xgb = xgboost(data=data.matrix(downsample.train[,1:29])
              ,label=as.numeric(downsample.train$Class)-1
              ,objective = "binary:logistic"
              ,max.depth = 2
              ,eta = 1
              ,nthread = 2
              ,nrounds = 25)

# Measure model performance on training set
pred = predict(xgb, data.matrix(downsample.train[,1:29]))
pred = as.numeric(pred>0.5)
print(head(pred))
training_accuracy = mean(pred==(as.numeric(downsample.train$Class)-1))
print(paste("Model accuracy on training set:", training_accuracy))

# Make predictions
predictions = predict(xgb, data.matrix(downsample.test[,1:29]))
length(predictions) == dim(downsample.test)[1]

# Transform predictions to binary results
predictions = as.numeric(predictions>0.5)
print(head(predictions))

# Measure model performance on test set
test_accuracy = mean(predictions==(as.numeric(downsample.test$Class)-1))
print(paste("Model accuracy on test set:", test_accuracy))
```






