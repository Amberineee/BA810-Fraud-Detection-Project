---
title: 'Team 5: Project Deliverable'
author: "Roozbeh Jafari, Jeffrey Leung, Yi-shuan Wang, Ying Wu, Yuzhe Zheng"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup
```{r}
library(data.table)
library(caTools)
library(xgboost)
library(caret)
library(ROCR)
library(pROC)
library(ROSE)
library(randomForest)
library(ggplot2)
library(dplyr)
library(solitude)

credit_card_raw = fread("creditcard.csv")
```

## Splitting Data
```{r}
# Create train and test dataset
credit_card_raw[, test:=0]
credit_card_raw[, "Time":= NULL]
credit_card_raw[sample(nrow(credit_card_raw), 284807*0.2), test:=1]
test <- credit_card_raw[test==1]
train <- credit_card_raw[test==0]
train[, "test" := NULL]
test[, "test" := NULL]
credit_card_raw[, "test" := NULL]

# Convert datatables to dataframes for downsampling
setDF(train)
setDF(test)

# Downsample
set.seed(1)
train$Class <- factor(train$Class)
downsample.train <- downSample(train[, -ncol(train)], train$Class)

test$Class <- factor(test$Class)
downsample.test <- downSample(test[, -ncol(test)], test$Class)
```

## Logistic Regression
```{r}
# Fit logistic regression model
set.seed(1)

#fit the model on balanced data(downsampling)
down_fit <- glm(Class ~ .,family = "binomial" ,data = downsample.train)
summary(down_fit,)

pred_down <- predict(down_fit, downsample.test) #balanced
print('Fitting downsampled model to test data')

#Evaluate model performance on test set
roc.curve(downsample.test$Class, pred_down, plotit=TRUE)

#predict on imbalanced test set
pred_imbalanced_down <- predict(down_fit, test) #imbalanced

roc.curve(test$Class, pred_imbalanced_down, plotit = TRUE)

###########
#fit the model on imbalanced data(downsampling)
org_fit <- glm(Class ~ .,family = "binomial" ,data = train)
summary(org_fit,)

pred_org <- predict(org_fit, downsample.test)
print('Fitting imbalanced model to downsample.test data')

#Evaluate model performance on test set
roc.curve(downsample.test$Class, pred_org, plotit=TRUE)

pred_imbalanced_org <- predict(org_fit, test)

#Evaluate model performance on test set
roc.curve(test$Class, pred_imbalanced_org, plotit=TRUE)
```

## Decision Tree
```{r}
# Fit decision tree model
#apply 5-folds cross validation to find the best parameter cp for decision tree
ctrl <- trainControl(method = "cv", number = 5)

#use downsample training set to fit model
dt <- train(Class ~ ., data = downsample.train,
               method = 'rpart',
               trControl = ctrl)
```
```{r}
#find best cp for decision model
#the best model is about cp = 0.015
#evaluate the best model using test data
pred <- predict(dt, downsample.test)

#performance
confusionMatrix(pred, downsample.test$Class, positive = '1')

#ROC curve
roc.curve(downsample.test$Class,pred , plotit=TRUE)
```
```{r}
#predict on imbalanced test set
pred.imbalanced <- predict(dt, test)

#performance
confusionMatrix(pred.imbalanced, test$Class, positive = '1')

#ROC curve
roc.curve(test$Class, pred.imbalanced, plotit = TRUE)
```


```{r}
#use imbalanced training set to fit model
dt_imbalanced <- train(Class ~ ., data = train,
               method = 'rpart',
               trControl = ctrl)
```
```{r}
pred <- predict(dt_imbalanced, downsample.test)

#performance
confusionMatrix(pred, downsample.test$Class, positive = '1')

#ROC curve
roc.curve(downsample.test$Class, pred, plotit=TRUE)
```
```{r}
#predict on imbalanced test set
pred.imbalanced <- predict(dt_imbalanced, test)

#performance
confusionMatrix(pred.imbalanced, test$Class, positive = '1')

#ROC curve
roc.curve(test$Class, pred.imbalanced, plotit = TRUE)
```
## Random Forest
```{r}
# Fit random forest model
fit_rndfor <- randomForest(downsample.train$Class~., data=downsample.train, ntree = 500, importance = TRUE)

varImpPlot(fit_rndfor)

#make predictions 
pd <- predict(fit_rndfor, downsample.train[,-ncol(downsample.train)])
table(observed = downsample.train[,ncol(downsample.train)], predicted = pd)

pd.test <- predict(fit_rndfor, downsample.test[,-ncol(downsample.test)])
table(observed = downsample.test[,ncol(downsample.test)], predicted = pd.test)

confusionMatrix(pd.test, downsample.test$Class, positive = "1")

pd.test.original <- predict(fit_rndfor, test[,-ncol(test)])
table(observed = test[,ncol(test)], predicted = pd.test.original)

confusionMatrix(pd.test.original, test$Class, positive = "1")

#Random Forest fit with original dataset
fit_rndfor_origin <- randomForest(train$Class~., data=train, ntree = 500, importance = TRUE)
varImpPlot(fit_rndfor_origin)

#make predictions
pd.test.original2 <- predict(fit_rndfor_origin, test[,-ncol(test)])
table(observed = test[,ncol(test)], predicted = pd.test.original2)

pd.test.original3 <- predict(fit_rndfor_origin, downsample.test[,-ncol(downsample.test)])
table(observed = downsample.test[,ncol(downsample.test)], predicted = pd.test.original3)

confusionMatrix(pd.test.original2, test$Class, positive = "1")
confusionMatrix(pd.test.original3, downsample.test$Class, positive = "1")

#ROC curve and AUC
roc.curve(downsample.test$Class, pd.test, plotit=TRUE)
roc.curve(test$Class, pd.test.original, plotit=TRUE)
roc.curve(test$Class, pd.test.original2, plotit=TRUE)
roc.curve(downsample.test$Class, pd.test.original3, plotit=TRUE)

#cross validation 
rf.cv <- rfcv(downsample.train[,-ncol(downsample.train)], downsample.train[,ncol(downsample.train)], cv.fold = 10) 
with(rf.cv, plot(n.var,error.cv))

#tuning parameter
bestmtry <- tuneRF(downsample.train[,-ncol(downsample.train)], downsample.train[,ncol(downsample.train)], stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry) #mtry = 4 gives an accuracy of 94%; mtry: Number of variables randomly sampled as candidates at each split.
```

## XGBoost
```{r}
# Cross-validation (downsample.train)
dtrain = data.matrix(downsample.train[,1:29])
best_param = list()
best_seednumber = 1234
best_auc = Inf
best_auc_index = 0

for (iter in 1:10) {
  param <- list(objective = "binary:logistic", eval_metric = "auc",
                max_depth = 2, eta = 1, nthread = 2
  )
  cv.nround = 1000
  cv.nfold = 5
  seed.number = sample.int(10000, 1)[[1]]
  set.seed(seed.number)
  mdcv <- xgb.cv(data=dtrain, params = param,
                 nfold=cv.nfold, nrounds=cv.nround, verbose=0,
                 early_stopping_rounds=8, maximize=TRUE,
                 label=as.numeric(downsample.train$Class)-1)
  
  min_auc = min(mdcv$evaluation_log[, test_auc_mean])
  min_auc_index = which.min(mdcv$evaluation_log[, test_auc_mean])
  
  if (min_auc < best_auc) {
    best_auc = min_auc
    best_auc_index = min_auc_index
    best_seednumber = seed.number
    best_param = param
  }
}

nround = best_auc
set.seed(best_seednumber)
```
```{r}
# Fit XGBoost model on downsampled training set
xgb = xgboost(data = dtrain,
              params = best_param, 
              nround = nround, 
              label=as.numeric(downsample.train$Class)-1)

# Apply XGBoost model on downsampled test set
predictions = predict(xgb, data.matrix(downsample.test[,1:29]))
# Transform predictions to binary results
predictions = as.numeric(predictions>0.5)
# Confusion matrix
cm1 = confusionMatrix(as.factor(predictions), downsample.test$Class
                      ,dnn=c("Prediction", "Reference"))
print(cm1)
# Plot ROC curve
roc1 = roc.curve(downsample.test$Class, as.factor(predictions), plotit = TRUE)
print(paste("Area under the curve (AUC):", round(roc1$auc, digits=3)))


# Apply XGBoost model on imbalanced test set
predictions2 = predict(xgb, data.matrix(test[,1:29]))
predictions2 = as.numeric(predictions2>0.5)
# Confusion matrix
cm2 = confusionMatrix(as.factor(predictions2), test$Class
                      ,dnn=c("Prediction", "Reference"))
print(cm2)
# Plot ROC curve
roc2 = roc.curve(test$Class, as.factor(predictions2), plotit = TRUE)
print(paste("Area under the curve (AUC):", round(roc2$auc, digits=3)))
```
```{r}
# Cross-validation (imbalanced training set)
dtrain2 = data.matrix(train[,1:29])
best_param2 = list()
best_seednumber2 = 1234
best_auc2 = Inf
best_auc_index2 = 0

for (iter in 1:10) {
  param <- list(objective = "binary:logistic", eval_metric = "auc",
                max_depth = 2, eta = 1, nthread = 2
  )
  cv.nround = 1000
  cv.nfold = 5
  seed.number = sample.int(10000, 1)[[1]]
  set.seed(seed.number)
  mdcv <- xgb.cv(data=dtrain2, params = param,
                 nfold=cv.nfold, nrounds=cv.nround, verbose=0,
                 early_stopping_rounds=8, maximize=TRUE,
                 label=as.numeric(train$Class)-1)
  
  min_auc = min(mdcv$evaluation_log[, test_auc_mean])
  min_auc_index = which.min(mdcv$evaluation_log[, test_auc_mean])
  
  if (min_auc < best_auc2) {
    best_auc2 = min_auc
    best_auc_index2 = min_auc_index
    best_seednumber2 = seed.number
    best_param2 = param
  }
}

nround2 = best_auc2
set.seed(best_seednumber2)
```
```{r}
# Fit XGBoost model on imbalanced training set
xgb2 = xgboost(data = dtrain2,
               params = best_param2, 
               nround = nround2, 
               label=as.numeric(train$Class)-1)


# Apply XGBoost model on downsampled test set
pred = predict(xgb2, data.matrix(downsample.test[,1:29]))
pred = as.numeric(pred>0.5)
# Confusion matrix
cm3 = confusionMatrix(as.factor(pred), downsample.test$Class
                      ,dnn=c("Prediction", "Reference"))
print(cm3)
# Plot ROC curve
roc3 = roc.curve(downsample.test$Class, as.factor(pred), plotit = TRUE)
print(paste("Area under the curve (AUC):", round(roc3$auc, digits=3)))


# Apply XGBoost model on imbalanced test set
pred2 = predict(xgb2, data.matrix(test[,1:29]))
pred2 = as.numeric(pred2>0.5)
# Confusion matrix
cm4 = confusionMatrix(as.factor(pred2), test$Class
                      ,dnn=c("Prediction", "Reference"))
print(cm4)
# Plot ROC curve
roc4 = roc.curve(test$Class, as.factor(pred2), plotit = TRUE)
print(paste("Area under the curve (AUC):", round(roc4$auc, digits=3)))
```
```{r}
# Model comparison
print(paste("Sensitivity:"
            , cm1$byClass["Sensitivity"]
            ,"Specificity:"
            , cm1$byClass["Specificity"]
            , " AUC:"
            , round(roc1$auc, digits=3)
            , "(downsampled training & downsampled test)"
))
print(paste("Sensitivity:"
            , cm2$byClass["Sensitivity"]
            ,"Specificity:"
            , cm2$byClass["Specificity"]
            , " AUC:"
            , round(roc2$auc, digits=3)
            ,"(downsampled training & imbalanced test)"
))
print(paste("Sensitivity:"
            , cm3$byClass["Sensitivity"]
            ,"Specificity:"
            , cm3$byClass["Specificity"]
            , " AUC:"
            , round(roc3$auc, digits=3)
            ,"(imbalanced training & downsampled test)"
))
print(paste("Sensitivity:"
            , cm4$byClass["Sensitivity"]
            ,"Specificity:"
            , cm4$byClass["Specificity"]
            , " AUC:"
            , round(roc4$auc, digits=3)
            ,"(imbalanced training & imbalanced test)"
))
```

## KNN
```{r}
# Set cross validation parameters
knn_ctrl <- trainControl(method = "cv", number = 5)

# Run model
knn_model <- train(Class ~ ., data = downsample.train, method = "knn", trControl = knn_ctrl)
```
```{r}
# Predict on downsampled test set
knn_prediction <- predict(knn_model, downsample.test)

# Confusion matrix
confusionMatrix(knn_prediction, downsample.test$Class, positive = "1")

#ROC curve on downsampled test data
roc.curve(downsample.test$Class, knn_prediction, plotit = TRUE)
```
```{r}
# Predict on imbalanced test set 
prediction.imbalanced <- predict(knn_model, test)

# Confusion matrix
confusionMatrix(prediction.imbalanced, test$Class, positive = "1")

#ROC curve on imbalanced test data
knn_prediction_imbalanced <- predict(knn_model, test)
roc.curve(test$Class, prediction.imbalanced, plotit = TRUE)
```

## Isolation Forest
```{r}
# Copy new data as to not disturb other models
iforest_train <- copy(train)
iforest_test <- copy(test)
```
```{r}
# initiate an isolation forest
iso <- isolationForest$new(sample_size = length(iforest_train))
# fit for data
iso$fit(iforest_train)
```
```{r}
# Obtain anomaly scores. Scores closer to 1 are likely outliers. 
# All scores hovering around 0.5 indicate low likelihood of outliers, so we set the threshold to 0.6.
iforest_scores_train = iso$predict(iforest_train)
iforest_scores_train[order(anomaly_score, decreasing = TRUE)]
iforest_train$predictions <- as.factor(ifelse(iforest_scores_train$anomaly_score >=0.6, 1, 0))

iforest_scores_test = iso$predict(iforest_test)
iforest_scores_test[order(anomaly_score, decreasing = TRUE)]
iforest_test$predictions <- as.factor(ifelse(iforest_scores_test$anomaly_score >=0.6, 1, 0))
```
```{r}
# Confusion Matrix and ROC curve of training data
confusionMatrix(iforest_train$predictions, as.factor(train$Class), positive = "1")
roc.curve(train$Class, iforest_train$predictions, plotit = TRUE)
```
```{r}
# Confusion Matrix and ROC curve of test data
confusionMatrix(iforest_test$predictions, as.factor(test$Class), positive = "1")
roc.curve(test$Class, iforest_test$predictions, plotit = TRUE)
```
