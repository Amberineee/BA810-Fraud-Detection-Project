---
title: 'Team 5: Project Deliverable'
author: "Mohsen Jafari, Jeffrey Leung, Yi-shuan Wang, Ying Wu, Yuzhe Zheng"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup
```{r}
library(data.table)
library(caTools)
library(xgboost)
library(caret)
library(ROCR)
library(pROC)
library(ROSE)
library(randomForest)
library(ggplot2)

credit_card_raw = fread("creditcard.csv")
```

## Splitting Data
```{r}
# Create train and test dataset
credit_card_raw[, test:=0]
credit_card_raw[, "Time":= NULL]
credit_card_raw[sample(nrow(credit_card_raw), 284807*0.2), test:=1]
test <- credit_card_raw[test==1]
train <- credit_card_raw[test==0]
train[, "test" := NULL]
test[, "test" := NULL]
credit_card_raw[, "test" := NULL]

# Convert datatables to dataframes for downsampling
setDF(train)
setDF(test)

# Downsample
set.seed(1)
train$Class <- factor(train$Class)
downsample.train <- downSample(train[, -ncol(train)], train$Class)

test$Class <- factor(test$Class)
downsample.test <- downSample(test[, -ncol(test)], test$Class)
```

## Logistic Regression
```{r}
# Fit logistic regression model
set.seed(1)

down_fit <- glm(Class ~ .,family = "binomial" ,data = downsample.train)
summary(down_fit,)

pred_down <- predict(down_fit, newdata = test)
print('Fitting downsampled model to test data')

#Evaluate model performance on test set
roc.curve(test$Class, pred_down, plotit = TRUE)

```

## Decision Tree
```{r}
# Fit decision tree model
#apply 5-folds cross validation to find the best parameter cp for decision tree
ctrl <- trainControl(method = "cv", number = 5)

dt <- train(Class ~ ., data = downsample.train,
               method = 'rpart',
               trControl = ctrl)

#find best cp for decision model
#the best model is about cp = 0.015
#evaluate the best model using test data
pred <- predict(dt, downsample.test)

#performance
confusionMatrix(pred, downsample.test$Class, positive = '1')

#ROC curve
prediction_for_roc_curve <- predict(dt,downsample.test[,-ncol(downsample.test)],type="prob")
pretty_colours <- c("#F8766D","#00BA38")
classes <- levels(test$Class)

for (i in 1:2)
{
  # Define which observations belong to class[i]
  true_values <- ifelse(downsample.test[,ncol(downsample.test)]==classes[i],1,0)
  # Assess the performance of classifier for class[i]
  pred <- prediction(prediction_for_roc_curve[,i],true_values)
  perf <- performance(pred, "tpr", "fpr")
  if (i==1)
  {
    plot(perf,main="ROC Curve",col=pretty_colours[i]) 
  }
  else
  {
    plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE) 
  }
   # Calculate the AUC and print it to screen
  auc.perf <- performance(pred, measure = "auc")
  print(auc.perf@y.values)
}

```

## Random Forest
```{r}
# Fit random forest model
fit_rndfor <- randomForest(downsample.train$Class~., data=downsample.train, ntree = 500, importance = TRUE)

varImpPlot(fit_rndfor)

#make predictions 
pd <- predict(fit_rndfor, downsample.train[,-ncol(downsample.train)])
table(observed = downsample.train[,ncol(downsample.train)], predicted = pd)

pd.test <- predict(fit_rndfor, downsample.test[,-ncol(downsample.test)])
table(observed = downsample.test[,ncol(downsample.test)], predicted = pd.test)

#ROC curve
prediction_for_roc_curve <- predict(fit_rndfor,downsample.test[,-ncol(downsample.test)],type="prob")


pred <- prediction(prediction_for_roc_curve,true_values)
perf <- performance(pred, "tpr", "fpr")
  
plot(perf,main="ROC Curve", colorize=TRUE) 

# Calculate the AUC and print it to screen
auc.perf <- performance(pred, measure = "auc")
print(auc.perf@y.values)

#cross validation 
rf.cv <- rfcv(downsample.train[,-ncol(downsample.train)], downsample.train[,ncol(downsample.train)], cv.fold = 10) 
with(rf.cv, plot(n.var,error.cv))

#tuning parameter
bestmtry <- tuneRF(downsample.train[,-ncol(downsample.train)], downsample.train[,ncol(downsample.train)], stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry) #mtry = 4 gives an accuracy of 94%; mtry: Number of variables randomly sampled as candidates at each split.
```

## XGBoost
```{r}
# Fit XGBoost model
xgb = xgboost(data=data.matrix(downsample.train[,1:29])
              ,label=as.numeric(downsample.train$Class)-1
              ,objective = "binary:logistic"
              ,max.depth = 2
              ,eta = 1
              ,nthread = 2
              ,nrounds = 25)

# Measure model performance on training set
pred = predict(xgb, data.matrix(downsample.train[,1:29]))
pred = as.numeric(pred>0.5)
training_accuracy = mean(pred==(as.numeric(downsample.train$Class)-1))
print(paste("Model accuracy on training set:", training_accuracy))
# Confusion matrix for training set
confusionMatrix(as.factor(pred), downsample.train$Class
                ,dnn=c("Prediction", "Reference"))
# Plot ROC curve
# pred_roc = prediction(pred, as.numeric(downsample.train$Class)-1)
# roc_train = performance(pred_roc, "tpr", "fpr")
# plot(roc_train, lwd=3, main="ROC Curve (Training Set)"
#      ,xlab="False Positive Rate", ylab="True Positive Rate")
roc.curve(downsample.train$Class, as.factor(pred), plotit = TRUE)
```
```{r}
# Apply XGBoost model on test set
predictions = predict(xgb, data.matrix(downsample.test[,1:29]))
# Transform predictions to binary results
predictions = as.numeric(predictions>0.5)
# Measure model performance on test set
test_accuracy = mean(predictions==(as.numeric(downsample.test$Class)-1))
print(paste("Model accuracy on test set:", test_accuracy))
# Confusion matrix for test set
confusionMatrix(as.factor(predictions), downsample.test$Class
                ,dnn=c("Prediction", "Reference"))
# Plot ROC curve
# predictions_roc = prediction(predictions, as.numeric(downsample.test$Class)-1)
# roc_test = performance(predictions_roc, "tpr", "fpr")
# plot(roc_test, lwd=3, main="ROC Curve (Test Set)"
#      ,xlab="False Positive Rate", ylab="True Positive Rate")
roc.curve(downsample.test$Class, as.factor(predictions), plotit = TRUE)
```
```{r}
# Apply XGBoost model on raw dataset
yhat = predict(xgb, data.matrix(credit_card_raw[,1:29]))
# Transform predictions to binary results
yhat = as.numeric(yhat>0.5)
# Measure model performance
raw_accuracy = mean(yhat==credit_card_raw$Class)
print(paste("Model accuracy on raw data:", raw_accuracy))
# Confusion matrix
confusionMatrix(as.factor(yhat), as.factor(credit_card_raw$Class)
                ,dnn=c("Prediction", "Reference"))
# Plot ROC curve
# yhat_roc = prediction(yhat, credit_card_raw$Class)
# roc_raw = performance(yhat_roc, "tpr", "fpr")
# plot(roc_raw, lwd=3, main="ROC Curve (Raw Dataset)"
#      ,xlab="False Positive Rate", ylab="True Positive Rate")
roc.curve(credit_card_raw$Class, yhat, plotit = TRUE)
```
```{r}
# Downsample the raw dataset: 492 frauds & 492 non-frauds
df = setDF(credit_card_raw)
df$Class = factor(df$Class)
downsample.df = downSample(df[,-ncol(df)], df$Class)
# XGBoost with cross-validation
xgb_cv = xgb.cv(data=data.matrix(downsample.df[,1:29])
                ,label=as.numeric(downsample.df$Class)-1
                ,objective = "binary:logistic"
                ,max.depth = 3
                ,eta = 1
                ,nthread = 2
                ,nrounds = 4
                ,nfold = 5
                ,metrics=list("rmse","auc"))
print(xgb_cv)
```






