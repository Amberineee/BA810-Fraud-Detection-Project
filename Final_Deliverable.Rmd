---
title: 'Team 5: Project Deliverable'
author: "Mohsen Jafari, Jeffrey Leung, Yi-shuan Wang, Ying Wu, Yuzhe Zheng"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup
```{r}
library(data.table)
library(caTools)
library(xgboost)
library(caret)
library(ROCR)

credit_card_raw = fread("creditcard.csv")
```

## Splitting Data
```{r}
# Create train and test dataset
credit_card_raw[, test:=0]
credit_card_raw[, "Time":= NULL]
credit_card_raw[sample(nrow(credit_card_raw), 284807*0.2), test:=1]
test <- credit_card_raw[test==1]
train <- credit_card_raw[test==0]
train[, "test" := NULL]
test[, "test" := NULL]
credit_card_raw[, "test" := NULL]

# Convert datatables to dataframes for downsampling
setDF(train)
setDF(test)

# Downsample
set.seed(1)
train$Class <- factor(train$Class)
downsample.train <- downSample(train[, -ncol(train)], train$Class)

test$Class <- factor(test$Class)
downsample.test <- downSample(test[, -ncol(test)], test$Class)
```

## Logistic Regression
```{r}
# Fit logistic regression model

```

## Decision Tree
```{r}
# Fit decision tree model
#apply 5-folds cross validation to find the best parameter cp for decision tree
ctrl <- trainControl(method = "cv", number = 5)

dt <- train(Class ~ ., data = downsample.train,
               method = 'rpart',
               trControl = ctrl)

#find best cp for decision model
#the best model is about cp = 0.015
#evaluate the best model using test data
pred <- predict(dt, downsample.test)

#performance
confusionMatrix(pred, downsample.test$Class, positive = '1')

#ROC curve
prediction_for_roc_curve <- predict(dt,downsample.test[,-ncol(downsample.test)],type="prob")
pretty_colours <- c("#F8766D","#00BA38")
classes <- levels(test$Class)

for (i in 1:2)
{
  # Define which observations belong to class[i]
  true_values <- ifelse(downsample.test[,ncol(downsample.test)]==classes[i],1,0)
  # Assess the performance of classifier for class[i]
  pred <- prediction(prediction_for_roc_curve[,i],true_values)
  perf <- performance(pred, "tpr", "fpr")
  if (i==1)
  {
    plot(perf,main="ROC Curve",col=pretty_colours[i]) 
  }
  else
  {
    plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE) 
  }
   # Calculate the AUC and print it to screen
  auc.perf <- performance(pred, measure = "auc")
  print(auc.perf@y.values)
}


```

## Random Forest
```{r}
# Fit random forest model

```

## XGBoost
```{r}
# Fit XGBoost model
xgb = xgboost(data=data.matrix(downsample.train[,1:29])
              ,label=as.numeric(downsample.train$Class)-1
              ,objective = "binary:logistic"
              ,max.depth = 2
              ,eta = 1
              ,nthread = 2
              ,nrounds = 25)

# Measure model performance on training set
pred = predict(xgb, data.matrix(downsample.train[,1:29]))
pred = as.numeric(pred>0.5)
print(head(pred))
training_accuracy = mean(pred==(as.numeric(downsample.train$Class)-1))
print(paste("Model accuracy on training set:", training_accuracy))

# Make predictions
predictions = predict(xgb, data.matrix(downsample.test[,1:29]))
length(predictions) == dim(downsample.test)[1]

# Transform predictions to binary results
predictions = as.numeric(predictions>0.5)
print(head(predictions))

# Measure model performance on test set
test_accuracy = mean(predictions==(as.numeric(downsample.test$Class)-1))
print(paste("Model accuracy on test set:", test_accuracy))
```






