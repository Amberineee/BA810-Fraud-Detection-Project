---
title: 'Team 5: Project Deliverable'
author: "Mohsen Jafari, Jeffrey Leung, Yi-shuan Wang, Ying Wu, Yuzhe Zheng"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup
```{r}
library(data.table)
library(caTools)
library(xgboost)
library(caret)
library(ROCR)
library(pROC)
library(ROSE)
library(randomForest)
library(ggplot2)
library(dplyr)
library(solitude)

credit_card_raw = fread("creditcard.csv")
```

## Splitting Data
```{r}
# Create train and test dataset
credit_card_raw[, test:=0]
credit_card_raw[, "Time":= NULL]
credit_card_raw[sample(nrow(credit_card_raw), 284807*0.2), test:=1]
test <- credit_card_raw[test==1]
train <- credit_card_raw[test==0]
train[, "test" := NULL]
test[, "test" := NULL]
credit_card_raw[, "test" := NULL]

# Convert datatables to dataframes for downsampling
setDF(train)
setDF(test)

# Downsample
set.seed(1)
train$Class <- factor(train$Class)
downsample.train <- downSample(train[, -ncol(train)], train$Class)

test$Class <- factor(test$Class)
downsample.test <- downSample(test[, -ncol(test)], test$Class)
```

## Logistic Regression
```{r}
# Fit logistic regression model
set.seed(1)

down_fit <- glm(Class ~ .,family = "binomial" ,data = downsample.train)
summary(down_fit,)

pred_down <- predict(down_fit, newdata = test)
print('Fitting downsampled model to test data')

#Evaluate model performance on test set
roc.curve(test$Class, pred_down, plotit=TRUE)

```

## Decision Tree
```{r}
# Fit decision tree model
#apply 5-folds cross validation to find the best parameter cp for decision tree
ctrl <- trainControl(method = "cv", number = 5)

#use downsample training set to fit model
dt <- train(Class ~ ., data = downsample.train,
               method = 'rpart',
               trControl = ctrl)
```
```{r}
#find best cp for decision model
#the best model is about cp = 0.015
#evaluate the best model using test data
pred <- predict(dt, downsample.test)

#performance
confusionMatrix(pred, downsample.test$Class, positive = '1')

#ROC curve
roc.curve(downsample.test$Class,pred , plotit=TRUE)
```
```{r}
#predict on imbalanced test set
pred.imbalanced <- predict(dt, test)

#performance
confusionMatrix(pred.imbalanced, test$Class, positive = '1')

#ROC curve
roc.curve(test$Class, pred.imbalanced, plotit = TRUE)
```


```{r}
#use imbalanced training set to fit model
dt_imbalanced <- train(Class ~ ., data = train,
               method = 'rpart',
               trControl = ctrl)
```
```{r}
pred <- predict(dt_imbalanced, downsample.test)

#performance
confusionMatrix(pred, downsample.test$Class, positive = '1')

#ROC curve
roc.curve(downsample.test$Class, pred, plotit=TRUE)
```
```{r}
#predict on imbalanced test set
pred.imbalanced <- predict(dt_imbalanced, test)

#performance
confusionMatrix(pred.imbalanced, test$Class, positive = '1')

#ROC curve
roc.curve(test$Class, pred.imbalanced, plotit = TRUE)
```
## Random Forest
```{r}
# Fit random forest model
fit_rndfor <- randomForest(downsample.train$Class~., data=downsample.train, ntree = 500, importance = TRUE)

varImpPlot(fit_rndfor)

#make predictions 
pd <- predict(fit_rndfor, downsample.train[,-ncol(downsample.train)])
table(observed = downsample.train[,ncol(downsample.train)], predicted = pd)

pd.test <- predict(fit_rndfor, downsample.test[,-ncol(downsample.test)])
table(observed = downsample.test[,ncol(downsample.test)], predicted = pd.test)

#ROC curve and AUC
roc.curve(downsample.test$Class, pd.test, plotit=TRUE)

#cross validation 
rf.cv <- rfcv(downsample.train[,-ncol(downsample.train)], downsample.train[,ncol(downsample.train)], cv.fold = 10) 
with(rf.cv, plot(n.var,error.cv))

#tuning parameter
bestmtry <- tuneRF(downsample.train[,-ncol(downsample.train)], downsample.train[,ncol(downsample.train)], stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry) #mtry = 4 gives an accuracy of 94%; mtry: Number of variables randomly sampled as candidates at each split.
```

## XGBoost
```{r}
# Fit XGBoost model
xgb = xgboost(data=data.matrix(downsample.train[,1:29])
              ,label=as.numeric(downsample.train$Class)-1
              ,objective = "binary:logistic"
              ,max.depth = 2
              ,eta = 1
              ,nthread = 2
              ,nrounds = 25)

# Measure model performance on training set
pred = predict(xgb, data.matrix(downsample.train[,1:29]))
pred = as.numeric(pred>0.5)
training_accuracy = mean(pred==(as.numeric(downsample.train$Class)-1))
print(paste("Model accuracy on training set:", training_accuracy))
# Confusion matrix for training set
confusionMatrix(as.factor(pred), downsample.train$Class
                ,dnn=c("Prediction", "Reference"))
# Plot ROC curve
roc.curve(downsample.train$Class, as.factor(pred), plotit = TRUE)
```
```{r}
# Apply XGBoost model on test set
predictions = predict(xgb, data.matrix(downsample.test[,1:29]))
# Transform predictions to binary results
predictions = as.numeric(predictions>0.5)
# Measure model performance on test set
test_accuracy = mean(predictions==(as.numeric(downsample.test$Class)-1))
print(paste("Model accuracy on test set:", test_accuracy))
# Confusion matrix for test set
confusionMatrix(as.factor(predictions), downsample.test$Class
                ,dnn=c("Prediction", "Reference"))
# Plot ROC curve
roc.curve(downsample.test$Class, as.factor(predictions), plotit = TRUE)
```
```{r}
# Apply XGBoost model on raw dataset
yhat = predict(xgb, data.matrix(credit_card_raw[,1:29]))
# Transform predictions to binary results
yhat = as.numeric(yhat>0.5)
# Measure model performance
raw_accuracy = mean(yhat==credit_card_raw$Class)
print(paste("Model accuracy on raw data:", raw_accuracy))
# Confusion matrix
confusionMatrix(as.factor(yhat), as.factor(credit_card_raw$Class)
                ,dnn=c("Prediction", "Reference"))
# Plot ROC curve
roc.curve(credit_card_raw$Class, yhat, plotit = TRUE)
```
```{r}
# Downsample the raw dataset: 492 frauds & 492 non-frauds
df = setDF(credit_card_raw)
df$Class = factor(df$Class)
downsample.df = downSample(df[,-ncol(df)], df$Class)
# XGBoost with cross-validation
xgb_cv = xgb.cv(data=data.matrix(downsample.df[,1:29])
                ,label=as.numeric(downsample.df$Class)-1
                ,objective = "binary:logistic"
                ,max.depth = 3
                ,eta = 1
                ,nthread = 2
                ,nrounds = 4
                ,nfold = 5
                ,metrics=list("rmse","auc"))
print(xgb_cv)
```

## KNN
```{r}
# Set cross validation parameters
knn_ctrl <- trainControl(method = "cv", number = 5)

# Run model
knn_model <- train(Class ~ ., data = downsample.train, method = "knn", trControl = knn_ctrl)
```
```{r}
# Predict on downsampled test set
knn_prediction <- predict(knn_model, downsample.test)

# Confusion matrix
confusionMatrix(knn_prediction, downsample.test$Class, positive = "1")

#ROC curve on downsampled test data
roc.curve(downsample.test$Class, knn_prediction, plotit = TRUE)
```
```{r}
# Predict on imbalanced test set 
prediction.imbalanced <- predict(knn_model, test)

# Confusion matrix
confusionMatrix(prediction.imbalanced, test$Class, positive = "1")

#ROC curve on imbalanced test data
knn_prediction_imbalanced <- predict(knn_model, test)
roc.curve(test$Class, prediction.imbalanced, plotit = TRUE)
```

## Isolation Forest
```{r}
# Copy new data as to not disturb other models
iforest_train <- copy(train)
iforest_test <- copy(test)
```
```{r}
# initiate an isolation forest
iso <- isolationForest$new(sample_size = length(iforest_train))
# fit for data
iso$fit(iforest_train)
```
```{r}
# Obtain anomaly scores. Scores closer to 1 are likely outliers. 
# All scores hovering around 0.5 indicate low likelihood of outliers, so we set the threshold to 0.6.
iforest_scores_train = iso$predict(iforest_train)
iforest_scores_train[order(anomaly_score, decreasing = TRUE)]
iforest_train$predictions <- as.factor(ifelse(iforest_scores_train$anomaly_score >=0.6, 1, 0))

iforest_scores_test = iso$predict(iforest_test)
iforest_scores_test[order(anomaly_score, decreasing = TRUE)]
iforest_test$predictions <- as.factor(ifelse(iforest_scores_test$anomaly_score >=0.6, 1, 0))
```
```{r}
# Confusion Matrix and ROC curve of training data
confusionMatrix(iforest_train$predictions, as.factor(train$Class), positive = "1")
roc.curve(train$Class, iforest_train$predictions, plotit = TRUE)
```
```{r}
# Confusion Matrix and ROC curve of test data
confusionMatrix(iforest_test$predictions, as.factor(test$Class), positive = "1")
roc.curve(test$Class, iforest_test$predictions, plotit = TRUE)
```
