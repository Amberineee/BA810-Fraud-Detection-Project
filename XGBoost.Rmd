```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Setup
```{r}
library(data.table)
library(caTools)
library(xgboost)

df = fread("creditcard.csv")
str(df)
```

### Summary Statistics
0.17% of the data was labeled as fraud.
```{r}
table(df$Class)
print(paste("% of fraud:", round(table(df$Class)[2]/nrow(df), 4)))
```

### Data Cleaning
```{r}
# remove uninformative feature
df[,Time:=NULL]

# make sure all features are numeric because XGBoost only works numeric vectors
df$Class = as.numeric(df$Class)
```

### Splitting Data
```{r}
set.seed(820)
split = sample.split(df$Class, SplitRatio=0.8)
train = as.matrix(subset(df, split==TRUE))
test = as.matrix(subset(df, split==FALSE))
```
```{r}
# training set shape
dim(train)
# test set shape
dim(test)

print(paste("% of fraud in training set:", round(table(train[,"Class"])[2]/nrow(train), 4)))
print(paste("% of fraud in test set:", round(table(test[,"Class"])[2]/nrow(test), 4)))
```

### XGBoost
```{r}
# train XGBoost model
xgb = xgboost(data=train[,1:29], label=train[,"Class"], objective = "binary:logistic",
              max.depth = 2, eta = 1, nthread = 2, nrounds = 25)
```
```{r}
# measure model performance on training set
pred = predict(xgb, train[,1:29])
pred = as.numeric(pred>0.5)
print(head(pred))
training_accuracy = mean(pred==train[,"Class"])
print(paste("Model accuracy on training set:", training_accuracy))
```
```{r}
# make predictions
predictions = predict(xgb, test[,1:29])
length(predictions) == dim(test)[1]

# transform predictions to binary results
predictions = as.numeric(predictions>0.5)
print(head(predictions))

# measure model performance on test set
test_accuracy = mean(predictions==test[,"Class"])
print(paste("Model accuracy on test set:", test_accuracy))
```
